{"pageProps":{"post":{"mdxSource":"var Component=(()=>{var l=Object.create;var s=Object.defineProperty;var c=Object.getOwnPropertyDescriptor;var m=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,g=Object.prototype.hasOwnProperty;var r=t=>s(t,\"__esModule\",{value:!0});var f=(t,a)=>()=>(a||t((a={exports:{}}).exports,a),a.exports),u=(t,a)=>{r(t);for(var n in a)s(t,n,{get:a[n],enumerable:!0})},w=(t,a,n)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let i of m(a))!g.call(t,i)&&i!==\"default\"&&s(t,i,{get:()=>a[i],enumerable:!(n=c(a,i))||n.enumerable});return t},b=t=>w(r(s(t!=null?l(p(t)):{},\"default\",t&&t.__esModule&&\"default\"in t?{get:()=>t.default,enumerable:!0}:{value:t,enumerable:!0})),t);var d=f((j,h)=>{h.exports=_jsx_runtime});var I={};u(I,{default:()=>v,frontmatter:()=>y});var e=b(d()),y={title:\"Posing Heads with Stable Diffusion\",date:new Date(1675006334e3),lastmod:\"2023-01-29\",tags:[\"Diffusion\",\"MID-U\",\"miniai\"],draft:!1,canonicalUrl:\"https://tailwind-nextjs-starter-blog.vercel.app/blog/new-features-in-v1/\"};function k(t={}){let{wrapper:a}=t.components||{};return a?(0,e.jsx)(a,Object.assign({},t,{children:(0,e.jsx)(n,{})})):n();function n(){let i=Object.assign({p:\"p\",strong:\"strong\",div:\"div\",h3:\"h3\",a:\"a\",span:\"span\",h4:\"h4\"},t.components),{Image:o}=i;return o||x(\"Image\",!0),(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)(i.p,{children:'This is a quick overview of my experiments using an Image Regression Model to guide head position, pose and scale of \"headshot\"-style images generated by Stable Diffusion.'}),(0,e.jsx)(i.p,{children:(0,e.jsx)(i.strong,{children:\"All with no fine-tuning of the Stable Diffusion model!\"})}),(0,e.jsx)(i.p,{children:\"In these experiments, I have not done any fine-tuning of the Stable Diffusion model. Rather I'm using my own image regression model (trained on a head pose dataset) to guide Stable Diffusion's image generation at inference time, operating in latent space rather than image space.\"}),(0,e.jsx)(i.div,{children:(0,e.jsx)(o,{alt:\"photo of a woman\",src:\"/static/images/posingHeads/sample1.jpeg\",width:\"832\",height:\"182\"})}),(0,e.jsx)(i.p,{children:\"This is built on top of a technique outlined in Jonathon Whitaker's ground-breaking article, Mid-U Guidance: Fast Classifier Guidance for Latent Diffusion Models. In this article, He describes a new technique for efficiently using an image classifier model to guide Stable Diffusion inference.\"}),(0,e.jsx)(i.p,{children:\"Latent diffusion models such as Stable Diffusion make it difficult to use image models for classifier guidance, since they operate internally on a highly compressed image represention (latents). Trying to use a classifier that operates in image space to steer the image generation process, would require tracing gradients not only through the classifier model, but also back through the decoder for the VAE and the upsampling path of Stable Diffusion's UNet making it very memory and compute intensive.\"}),(0,e.jsx)(i.p,{children:\"The magic of JohnO's approach is his realization that the Stable Diffusion encoder is a very powerful feature extractor and if there was a way to intercept the feature map from the encoder that this could be used as the backbone of a classifier model that operates in latent space rather than image space.\"}),(0,e.jsx)(i.div,{children:(0,e.jsx)(o,{alt:\"midu\",src:\"/static/images/posingHeads/midu.jpeg\",width:\"736\",height:\"396\"})}),(0,e.jsx)(i.p,{children:'This diagram[1] above illustrates the UNet from Stable Diffusion. The blocks with the blue background show the \"encoder\" (downsampling network) and the orange blocks show the decoder (upsampling network). The block in the middle is called the mid-block and is the point at which the encoder and decoder meet.'}),(0,e.jsx)(i.p,{children:\"JohnO descibes using a PyTorch hook to intercept the feature map from the mid-block of Stable Diffusions UNet giving us a (1280,8,8) compact but rich feature map upon which to train our custom model.\"}),(0,e.jsx)(i.p,{children:\"For my experiments, I built an image regression model that I've trained to predict head position, orientation and scale from input images.\"}),(0,e.jsx)(i.p,{children:\"Later I'm able to use this model at Stable Diffusion inference time to specify numeric targets for head position, orientation and scale.\"}),(0,e.jsxs)(i.h3,{id:\"sample-images\",children:[(0,e.jsx)(i.a,{href:\"#sample-images\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(i.span,{className:\"icon icon-link\"})}),\"Sample Images\"]}),(0,e.jsx)(i.p,{children:\"In these sample images, I\\u2019m using my model to steer Stable Diffusion to center the generated headshot and to \\u201Cpose\\u201D the generated image in a given direction not with a text prompt, but by using my model to steer the orientation using numeric targets for pitch, yaw, x, y and scale.\"}),(0,e.jsx)(i.p,{children:\"Interpolation in pitch and yaw space with specified centered head position, I can generate a wide variety of head poses.\"}),(0,e.jsx)(i.div,{children:(0,e.jsx)(o,{alt:\"grid\",src:\"/static/images/posingHeads/grid2.jpeg\",width:\"1500\",height:\"1500\"})}),(0,e.jsx)(i.p,{children:\"For demonstration purposes, I'm mapping these numeric pose targets to one of 'left', 'right', 'up', 'down', and 'front'.\"}),(0,e.jsx)(i.div,{children:(0,e.jsx)(o,{alt:\"sample2\",src:\"/static/images/posingHeads/sample2.jpeg\",width:\"832\",height:\"182\"})}),(0,e.jsx)(i.p,{children:\"Prompt: 'Photo of a woman'\"}),(0,e.jsx)(i.div,{children:(0,e.jsx)(o,{alt:\"sample3\",src:\"/static/images/posingHeads/sample3.jpeg\",width:\"832\",height:\"182\"})}),(0,e.jsx)(i.p,{children:\"Prompt: \\u201Cmagazine with a woman on the cover\\u201D\"}),(0,e.jsx)(i.div,{children:(0,e.jsx)(o,{alt:\"sample4\",src:\"/static/images/posingHeads/sample4.jpeg\",width:\"832\",height:\"182\"})}),(0,e.jsx)(i.p,{children:\"Prompt: \\u201Cphoto of a man\\u201D\"}),(0,e.jsx)(i.div,{children:(0,e.jsx)(o,{alt:\"sample5\",src:\"/static/images/posingHeads/sample5.jpeg\",width:\"832\",height:\"182\"})}),(0,e.jsx)(i.p,{children:\"Prompt: \\u201Cphoto of a man\\u201D\"}),(0,e.jsxs)(i.h3,{id:\"afterword\",children:[(0,e.jsx)(i.a,{href:\"#afterword\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(i.span,{className:\"icon icon-link\"})}),\"Afterword\"]}),(0,e.jsx)(i.p,{children:\"This is a very rich space to explore and I have a ton of other related research ideas.\"}),(0,e.jsxs)(i.p,{children:[\"Three months ago I knew nothing about Stable Diffusion. \",(0,e.jsx)(i.a,{href:\"https://twitter.com/jeremyphoward\",children:\"Jeremy Howards's\"}),\" latest course, \",(0,e.jsx)(i.a,{href:\"https://www.fast.ai/posts/part2-2022.html\",children:'\"From Deep Learning Foundations to Stable Diffusion\"'}),\" gave me not only an intuition about how this technology works but also imparted a strong foundational knowledge of its inner workings. I highly recommend it.\"]}),(0,e.jsx)(i.p,{children:`During his course, Jeremy started cranking out piece-by-piece a new \"experimental\" machine learning library called miniai. I used miniai to train the model that I described in this article. It's super flexible and its modest size make it a great platform for experimentation.`}),(0,e.jsxs)(i.h4,{id:\"references\",children:[(0,e.jsx)(i.a,{href:\"#references\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(i.span,{className:\"icon icon-link\"})}),\"References\"]}),(0,e.jsxs)(i.p,{children:[\"[1] \",(0,e.jsx)(i.a,{href:\"https://wandb.ai/johnowhitaker/midu-guidance/reports/Mid-U-Guidance-Fast-Classifier-Guidance-for-Latent-Diffusion-Models--VmlldzozMjg0NzA1\",children:\"Mid-U Guidance: Fast Classifier Guidance for Latent Diffusion Models\"}),\" by Jonathon Whitaker 2023\"]}),(0,e.jsxs)(i.p,{children:[\"For all commercial inquiries, please contact [\",(0,e.jsx)(i.a,{href:\"mailto:sales@liquidthought.com\",children:\"sales@liquidthought.com\"}),\"](mailto: \",(0,e.jsx)(i.a,{href:\"mailto:sales@liquidthought.com\",children:\"sales@liquidthought.com\"}),\")\"]})]})}}var v=k;function x(t,a){throw new Error(\"Expected \"+(a?\"component\":\"object\")+\" `\"+t+\"` to be defined: you likely forgot to import, pass, or provide it.\")}return I;})();\n;return Component;","toc":[{"value":"Sample Images","url":"#sample-images","depth":3},{"value":"Afterword","url":"#afterword","depth":3},{"value":"References","url":"#references","depth":4}],"frontMatter":{"readingTime":{"text":"4 min read","minutes":3.31,"time":198600,"words":662},"slug":"posing-heads-with-stable-diffusion","fileName":"posing-heads-with-stable-diffusion.md","title":"Posing Heads with Stable Diffusion","date":"2023-01-29T15:32:14.000Z","lastmod":"2023-01-29","tags":["Diffusion","MID-U","miniai"],"draft":false,"canonicalUrl":"https://tailwind-nextjs-starter-blog.vercel.app/blog/new-features-in-v1/"}},"authorDetails":[{"readingTime":{"text":"1 min read","minutes":0.495,"time":29700,"words":99},"slug":["default"],"fileName":"default.md","name":"John Robinson","avatar":"/static/images/avatar.jpeg","occupation":"Software Engineering & Architecture Executive","company":"Consultant for LiquidThought LLC","email":"johnrobinsn@gmail.com","github":"https://github.com/johnrobinsn","twitter":"https://twitter.com/johnrobinsn","linkedin":"https://www.linkedin.com/in/johnrobinsonsprofile/","date":null}],"prev":{"title":"Object Detection from Scratch - Part 1","date":"2023-01-12T15:32:14.000Z","lastmod":"2023-01-29","tags":["object detection","code"],"draft":false,"slug":"01_classification"},"next":null},"__N_SSG":true}