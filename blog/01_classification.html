<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><script>!function(){try {var d=document.documentElement.classList;d.remove('light','dark');var e=localStorage.getItem('theme');if("system"===e||(!e&&true)){var t="(prefers-color-scheme: dark)",m=window.matchMedia(t);m.media!==t||m.matches?d.add('dark'):d.add('light')}else if(e) d.add(e)}catch(e){}}()</script><meta content="width=device-width, initial-scale=1" name="viewport"/><title>Object Detection from Scratch - Part 1</title><meta name="robots" content="follow, index"/><meta name="description"/><meta property="og:url" content="https://tailwind-nextjs-starter-blog.vercel.app/blog/01_classification"/><meta property="og:type" content="article"/><meta property="og:site_name" content="Stormin&#x27; the Castle Blog "/><meta property="og:description"/><meta property="og:title" content="Object Detection from Scratch - Part 1"/><meta property="og:image" content="https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:site" content="https://twitter.com/johnrobinsn"/><meta name="twitter:title" content="Object Detection from Scratch - Part 1"/><meta name="twitter:description"/><meta name="twitter:image" content="https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png"/><link rel="canonical" href="https://tailwind-nextjs-starter-blog.vercel.app/blog/01_classification"/><meta property="article:published_time" content="2023-01-12T15:32:14.000Z"/><meta property="article:modified_time" content="2023-01-29T00:00:00.000Z"/><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://tailwind-nextjs-starter-blog.vercel.app/blog/01_classification"
  },
  "headline": "Object Detection from Scratch - Part 1",
  "image": [
    {
      "@type": "ImageObject",
      "url": "https://tailwind-nextjs-starter-blog.vercel.app/static/images/twitter-card.png"
    }
  ],
  "datePublished": "2023-01-12T15:32:14.000Z",
  "dateModified": "2023-01-29T00:00:00.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "John Robinson"
    }
  ],
  "publisher": {
    "@type": "Organization",
    "name": "John Robinson",
    "logo": {
      "@type": "ImageObject",
      "url": "https://tailwind-nextjs-starter-blog.vercel.app/static/images/logo.png"
    }
  }
}</script><meta name="next-head-count" content="21"/><link rel="apple-touch-icon" sizes="76x76" href="/static/favicons/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="/static/favicons/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/static/favicons/favicon-16x16.png"/><link rel="manifest" href="/static/favicons/site.webmanifest"/><link rel="mask-icon" href="/static/favicons/safari-pinned-tab.svg" color="#5bbad5"/><meta name="msapplication-TileColor" content="#000000"/><meta name="theme-color" media="(prefers-color-scheme: light)" content="#fff"/><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000"/><link rel="alternate" type="application/rss+xml" href="/feed.xml"/><link rel="preload" href="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/css/574139a4ce17f3a0.css" as="style"/><link rel="stylesheet" href="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/css/574139a4ce17f3a0.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/chunks/webpack-b489885ad711e1b0.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/chunks/main-bfc1a3b6dd286443.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/chunks/pages/_app-69631691045385fa.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/chunks/675-4efee9c9b489918b.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/chunks/410-35c4fb535edd9439.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/chunks/712-1460c880a709bdc7.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/chunks/pages/blog/%5B...slug%5D-8ab66ff6ebabec4c.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/mTxI35AS32IW1tSwy78Sx/_buildManifest.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/mTxI35AS32IW1tSwy78Sx/_ssgManifest.js" defer=""></script><script src="https://pinkdragon1000.github.io/StorminTheCastle/_next/static/mTxI35AS32IW1tSwy78Sx/_middlewareManifest.js" defer=""></script></head><body class="bg-white text-black antialiased dark:bg-gray-900 dark:text-white"><div id="__next" data-reactroot=""><div class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="flex h-screen flex-col justify-between"><header class="flex items-center justify-between py-10"><div><a aria-label="Stormin&#x27; the Castle Blog" href="/"><div class="flex items-center justify-between"><div class="mr-3"><svg width="101" height="83" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M34.082 19.243h33.035c.45 0 .865-.199 1.07-.514.204-.315.162-.693-.11-.976L51.566.378C51.336.141 50.981 0 50.601 0s-.74.141-.964.378L33.12 17.753c-.271.283-.313.66-.11.976.205.315.62.514 1.071.514Z" fill="url(#logo_svg__a)"></path><path d="M100.973 31.322c0-.744-.775-1.348-1.731-1.348h-3.327c-.947 0-1.718.593-1.73 1.331l-.049 2.9h-4.07v-2.882c0-.745-.774-1.349-1.73-1.349h-3.377c-.956 0-1.731.604-1.731 1.349v2.882h-4.062v-2.882c0-.745-.775-1.349-1.731-1.349h-3.383c-.955 0-1.73.603-1.73 1.347l-.017 12.354h-7.976V22.9c0-.563-.584-1.019-1.306-1.019h-24.84c-.722 0-1.306.456-1.306 1.02v20.775h-7.91l.01-12.351c0-.358-.182-.701-.506-.954-.325-.253-.765-.396-1.224-.396h-3.45c-.957 0-1.732.604-1.732 1.35v2.881h-4.062v-2.882c0-.745-.775-1.349-1.731-1.349h-3.377c-.956 0-1.731.604-1.731 1.35v2.881h-4.07v-2.882c0-.745-.775-1.349-1.73-1.349h-3.38c-.955 0-1.73.603-1.73 1.348L.187 83H37.64v-8.712c0-5.575 5.8-10.094 12.953-10.094 7.152 0 12.953 4.519 12.953 10.094V83H101l-.027-51.678ZM19.164 53.696c0 .564-.584 1.02-1.306 1.02h-6.547c-.722 0-1.306-.456-1.306-1.02V43.53c0-1.97 2.049-3.568 4.58-3.568 2.527 0 4.58 1.598 4.58 3.568v10.167Zm36.017-13.984c0 .564-.585 1.02-1.306 1.02h-6.547c-.722 0-1.306-.456-1.306-1.02V29.545c0-1.97 2.048-3.569 4.579-3.569 2.527 0 4.58 1.598 4.58 3.569v10.167Zm36.051 13.984c0 .564-.585 1.02-1.306 1.02H83.38c-.722 0-1.306-.456-1.306-1.02V43.53c0-1.97 2.049-3.568 4.58-3.568 2.527 0 4.58 1.598 4.58 3.568v10.167Z" fill="url(#logo_svg__b)"></path><defs><linearGradient id="logo_svg__a" x1="62.43" y1="1.314" x2="42.431" y2="30.626" gradientUnits="userSpaceOnUse"><stop stop-color="#85E2F4"></stop><stop offset="1" stop-color="#56B7D2"></stop></linearGradient><linearGradient id="logo_svg__b" x1="84.256" y1="26.055" x2="18.49" y2="112.405" gradientUnits="userSpaceOnUse"><stop stop-color="#85E2F4"></stop><stop offset="1" stop-color="#56B7D2"></stop></linearGradient></defs></svg></div><div class="hidden h-6 text-2xl font-semibold sm:block">Stormin&#x27; the Castle Blog</div></div></a></div><div class="flex items-center text-base leading-5"><div class="hidden sm:block"><a class="p-1 font-medium text-gray-900 dark:text-gray-100 sm:p-4" href="/tags">Tags</a><a class="p-1 font-medium text-gray-900 dark:text-gray-100 sm:p-4" href="/about">About</a></div><button aria-label="Toggle Dark Mode" type="button" class="ml-1 mr-1 h-8 w-8 rounded p-1 sm:ml-4"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001 0 1010.586 10.586z"></path></svg></button><div class="sm:hidden"><button type="button" class="ml-1 mr-1 h-8 w-8 rounded py-1" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"></path></svg></button><div class="fixed top-0 left-0 z-10 h-full w-full transform bg-gray-200 opacity-95 duration-300 ease-in-out dark:bg-gray-800 translate-x-full"><div class="flex justify-end"><button type="button" class="mr-5 mt-11 h-8 w-8 rounded" aria-label="Toggle Menu"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 dark:text-gray-100"><path fill-rule="evenodd" d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg></button></div><nav class="fixed mt-8 h-full"><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/tags">Tags</a></div><div class="px-12 py-4"><a class="text-2xl font-bold tracking-widest text-gray-900 dark:text-gray-100" href="/about">About</a></div></nav></div></div></div></header><main class="mb-auto"><div class="mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0"><div class="fixed right-8 bottom-8 hidden flex-col gap-3 md:hidden"><button aria-label="Scroll To Comment" type="button" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M18 10c0 3.866-3.582 7-8 7a8.841 8.841 0 01-4.083-.98L2 17l1.338-3.123C2.493 12.767 2 11.434 2 10c0-3.866 3.582-7 8-7s8 3.134 8 7zM7 9H5v2h2V9zm8 0h-2v2h2V9zM9 9h2v2H9V9z" clip-rule="evenodd"></path></svg></button><button aria-label="Scroll To Top" type="button" class="rounded-full bg-gray-200 p-2 text-gray-500 transition-all hover:bg-gray-300 dark:bg-gray-700 dark:text-gray-400 dark:hover:bg-gray-600"><svg class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M3.293 9.707a1 1 0 010-1.414l6-6a1 1 0 011.414 0l6 6a1 1 0 01-1.414 1.414L11 5.414V17a1 1 0 11-2 0V5.414L4.707 9.707a1 1 0 01-1.414 0z" clip-rule="evenodd"></path></svg></button></div><article><div class="xl:divide-y xl:divide-gray-200 xl:dark:divide-gray-700"><header class="pt-6 xl:pb-6"><div class="space-y-1 text-center"><dl class="space-y-10"><div><dt class="sr-only">Published on</dt><dd class="text-base font-medium leading-6 text-gray-500 dark:text-gray-400"><time dateTime="2023-01-12T15:32:14.000Z">Thursday, January 12, 2023</time></dd></div></dl><div><h1 class="text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14">Object Detection from Scratch - Part 1</h1></div></div></header><div class="divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:grid xl:grid-cols-4 xl:gap-x-6 xl:divide-y-0" style="grid-template-rows:auto 1fr"><dl class="pt-6 pb-10 xl:border-b xl:border-gray-200 xl:pt-11 xl:dark:border-gray-700"><dt class="sr-only">Authors</dt><dd><ul class="flex justify-center space-x-8 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8"><li class="flex items-center space-x-2"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2738%27%20height=%2738%27/%3e"/></span><img alt="avatar" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="h-10 w-10 rounded-full" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%"/><noscript><img alt="avatar" srcSet="/static/images/avatar.jpeg?imwidth=48 1x, /static/images/avatar.jpeg?imwidth=96 2x" src="/static/images/avatar.jpeg?imwidth=96" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="h-10 w-10 rounded-full" loading="lazy"/></noscript></span><dl class="whitespace-nowrap text-sm font-medium leading-5"><dt class="sr-only">Name</dt><dd class="text-gray-900 dark:text-gray-100">John Robinson</dd><dt class="sr-only">Twitter</dt><dd><a target="_blank" rel="noopener noreferrer" href="https://twitter.com/johnrobinsn" class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400">@johnrobinsn</a></dd></dl></li></ul></dd></dl><div class="divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0"><div class="prose max-w-none pt-10 pb-8 dark:prose-dark"><img src="https://www.storminthecastle.com/img/01_classification_files/computervision1_256.jpeg"/><br/><p><a target="_blank" rel="noopener noreferrer" href="https://github.com/johnrobinsn/ObjectDetectionNotebooks/blob/main/nbs/01_classification.ipynb"><img src="https://www.storminthecastle.com/img/github.svg"/></a><a target="_blank" rel="noopener noreferrer" href="https://colab.research.google.com/github/johnrobinsn/ObjectDetectionNotebooks/blob/main/nbs/01_classification.ipynb"><img src="https://www.storminthecastle.com/img/colab.svg"/></a></p><p>This is the start of my new series, &quot;Object Detection from Scratch&quot;, which is focused on building an intuition for how single-pass object detectors such as YOLO and SSD work. Object detection considers the problem of building a model that can take an image and detect multiple objects within that image; predicting not only the object classes but also the bounding boxes of those objects within the image. Single-pass detectors such as YOLO and SSD have solved this problem end-to-end performing the object detection task in a single forward inference pass of the model. If what I&#x27;m describing is not clear, here is a fun video of <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=MPU2HistivI">YOLO in action</a>.</p><p>In this series, I will incrementally build up a YOLO/SSD (Single Shot Detector) object model with just PyTorch and the current version of the FastAI 2 library. Both SSD and YOLO allow for single pass inference and can run efficiently on fairly low-end hardware allowing for realtime object detection for video content etc.</p><p>In order to learn about object detection we&#x27;ll need a dataset. We&#x27;ll be focused on using the <a target="_blank" rel="noopener noreferrer" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html">Pascal VOC dataset (2007)</a>. In total this dataset includes almost 10k images, containing about 24k annotated objects covering 20 different object classes. Each image is annotated with some number of object class labels and their cooresponding bounding boxes.</p><p>The notebooks in this series are designed to be easily used within Google Colab (free) or if you have your own GPU I&#x27;d recommend using conda to setup an environment with Python &gt;3.9 and pip install fastai.</p><p>To easily open this notebook just click on the &quot;Open in Colab&quot; button above.</p><p>The FastAI (FAI) library makes it easy to download the Pascal VOC dataset and access the object annotations.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">from</span> fastai<span class="token punctuation">.</span>vision<span class="token punctuation">.</span><span class="token builtin">all</span> <span class="token keyword">import</span> <span class="token operator">*</span>
</span></code></pre></div><div class="relative"><pre><code class="code-highlight"><span class="code-line">/home/jr/anaconda3/envs/fastaip2_3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory
</span><span class="code-line">warn(f&quot;Failed to load image Python extension: {e}&quot;)
</span></code></pre></div><p>Download the dataset using FAI&#x27;s untar_data function and take a quick look at the files that have been downloaded.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">path <span class="token operator">=</span> untar_data<span class="token punctuation">(</span>URLs<span class="token punctuation">.</span>PASCAL_2007<span class="token punctuation">)</span>
</span><span class="code-line">path<span class="token punctuation">.</span>ls<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span></code></pre></div><p><code>(#8) [Path(&#x27;/home/jr/.fastai/data/pascal_2007/train.json&#x27;),Path(&#x27;/home/jr/.fastai/data/pascal_2007/test.csv&#x27;),Path(&#x27;/home/jr/.fastai/data/pascal_2007/train.csv&#x27;),Path(&#x27;/home/jr/.fastai/data/pascal_2007/train&#x27;),Path(&#x27;/home/jr/.fastai/data/pascal_2007/test.json&#x27;),Path(&#x27;/home/jr/.fastai/data/pascal_2007/valid.json&#x27;),Path(&#x27;/home/jr/.fastai/data/pascal_2007/segmentation&#x27;),Path(&#x27;/home/jr/.fastai/data/pascal_2007/test&#x27;)]</code></p><h2 id="exploring-the-dataset"><a href="#exploring-the-dataset" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Exploring the Dataset</h2><p>FAI includes a get_annotations function that can parse COCO style bounding box annotations which the VOC dataset uses. Here we&#x27;ll just load the data associated with the train set (train.json). The function will return a tuple of lists. The first list will contain the image file names in the training set. The second list will contain cooresponding bounding boxes and object class labels for those images.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">imgs<span class="token punctuation">,</span>lbl_bbox <span class="token operator">=</span> get_annotations<span class="token punctuation">(</span>path<span class="token operator">/</span><span class="token string">&#x27;train.json&#x27;</span><span class="token punctuation">)</span>
</span><span class="code-line"><span class="token builtin">len</span><span class="token punctuation">(</span>imgs<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>lbl_bbox<span class="token punctuation">)</span>
</span></code></pre></div><p><code>(2501, 2501)</code></p><p>The dataset provides over 2500 training labeled (classes and bounding boxes). Here is a single example that demonstrates an image that has more than one object annotated. Showing two bounding boxes in image-space coordinates along with their respective object class labels.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">imgs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>lbl_bbox<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment"># show a single example</span>
</span></code></pre></div><p><code>(&#x27;000017.jpg&#x27;, ([[184, 61, 279, 199], [89, 77, 403, 336]], [&#x27;person&#x27;, &#x27;horse&#x27;]))</code></p><p>The bounding boxes consist of four numbers, the first two numbers represent the xy coordinates of the upper left corner of the bounding box and the second two numbers represent the xy coordinates of the lower right corner of the bounding box.</p><h2 id="visualize-the-dataset"><a href="#visualize-the-dataset" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Visualize the Dataset</h2><p>Using the matplotlib library let&#x27;s visualize an example from the training set.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>colors <span class="token keyword">as</span> mcolors
</span><span class="code-line"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>cm <span class="token keyword">as</span> cmx
</span><span class="code-line"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> patches<span class="token punctuation">,</span> patheffects
</span></code></pre></div><p>Let&#x27;s grab a sample out of the training set to explore.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># grab our example</span>
</span><span class="code-line">img_file<span class="token punctuation">,</span>img_bbox <span class="token operator">=</span> imgs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>lbl_bbox<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
</span><span class="code-line">img_file<span class="token punctuation">,</span>img_bbox
</span></code></pre></div><p><code>(&#x27;000017.jpg&#x27;, ([[184, 61, 279, 199], [89, 77, 403, 336]], [&#x27;person&#x27;, &#x27;horse&#x27;]))</code></p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># load the image using PIL</span>
</span><span class="code-line">img <span class="token operator">=</span> Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>path<span class="token operator">/</span><span class="token string-interpolation"><span class="token string">f&#x27;train/</span><span class="token interpolation"><span class="token punctuation">{</span>img_file<span class="token punctuation">}</span></span><span class="token string">&#x27;</span></span><span class="token punctuation">)</span>
</span><span class="code-line">img
</span></code></pre></div><p><img alt="png" src="https://www.storminthecastle.com/img/01_classification_files/01_classification_14_0.png"/></p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">h<span class="token punctuation">,</span>w <span class="token operator">=</span> img<span class="token punctuation">.</span>shape
</span><span class="code-line">h<span class="token punctuation">,</span>w <span class="token comment"># the height and width of the image; Note that height is typically specified first for numpy and PIL</span>
</span></code></pre></div><p><code>(364, 480)</code></p><p>The images in the dataset come in different sizes and shapes (but all 3 channel RGB images). In order to be able to use a GPU to accelerate training, we will want to work with a batch of images all in one go. This means that all of our images will need to be of the same size so that we can stack a batch into a single tensor. A fairly typical size to work with is 224x224 pixels. One important consideration is that since our bounding boxes are defined in image space coordinates, we really don&#x27;t want to crop the images since we might cut off our bounding boxes, therefore we opt to squish the images instead.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">SIZE<span class="token operator">=</span><span class="token number">224</span>
</span></code></pre></div><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># resample/rescale the example image by squishing</span>
</span><span class="code-line">img_scaled <span class="token operator">=</span> img<span class="token punctuation">.</span>resize<span class="token punctuation">(</span><span class="token punctuation">(</span>SIZE<span class="token punctuation">,</span>SIZE<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line">img_scaled
</span></code></pre></div><p><img alt="png" src="https://www.storminthecastle.com/img/01_classification_files/01_classification_18_0.png"/></p><p>Given the task of object detection our independent variable are the images and the dependent variables are the object classes and the object bounding boxes. Given an image, we will want our model to predict a class label for each object in the image along with a bounding box for each object. One wrinkle however is that our bounding box (dependent variable) is defined in the coordinate space of our image (independent variable). So if we scale the input image we must also be careful to scale the bounding boxes along with it. Since the image has been squished to be of size 224x224 pixels we need to squish (scale) the dimensions our bounding boxes for that sample by the same amount. Here I show the steps required to squish the dimensions of the bounding boxes for our sample.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># calculate how how much we&#x27;ve scaled each dimension of the image</span>
</span><span class="code-line">yscale<span class="token punctuation">,</span>xscale <span class="token operator">=</span> h<span class="token operator">/</span>SIZE<span class="token punctuation">,</span>w<span class="token operator">/</span>SIZE
</span><span class="code-line">yscale<span class="token punctuation">,</span>xscale
</span></code></pre></div><p><code>(1.625, 2.142857142857143)</code></p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># apply the same scale factor to each coordinate of the bounding box</span>
</span><span class="code-line">img_bbox_scaled <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>x1<span class="token operator">//</span>xscale<span class="token punctuation">,</span>y1<span class="token operator">//</span>yscale<span class="token punctuation">,</span>x2<span class="token operator">//</span>xscale<span class="token punctuation">,</span>y2<span class="token operator">//</span>yscale<span class="token punctuation">]</span> <span class="token keyword">for</span> x1<span class="token punctuation">,</span>y1<span class="token punctuation">,</span>x2<span class="token punctuation">,</span>y2 <span class="token keyword">in</span> img_bbox<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</span><span class="code-line">img_bbox_scaled
</span></code></pre></div><p><code>[[85.0, 37.0, 130.0, 122.0], [41.0, 47.0, 188.0, 206.0]]</code></p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># reconstruct the labled set of bounding boxes with the scaled bounding boxes for our example</span>
</span><span class="code-line">img_bbox_scaled <span class="token operator">=</span> <span class="token punctuation">(</span>img_bbox_scaled<span class="token punctuation">,</span>img_bbox<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">img_bbox_scaled
</span></code></pre></div><p><code>([[85.0, 37.0, 130.0, 122.0], [41.0, 47.0, 188.0, 206.0]], [&#x27;person&#x27;, &#x27;horse&#x27;])</code></p><p>Here is a small utility function for displaying an image in such a way that we can layer on some additional annotations.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">def</span> <span class="token function">show_img</span><span class="token punctuation">(</span>im<span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> ax<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    <span class="token keyword">if</span> <span class="token keyword">not</span> ax<span class="token punctuation">:</span> fig<span class="token punctuation">,</span>ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span>figsize<span class="token punctuation">)</span>
</span><span class="code-line">    ax<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>im<span class="token punctuation">)</span>
</span><span class="code-line">    ax<span class="token punctuation">.</span>set_xticks<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line">    ax<span class="token punctuation">.</span>set_yticks<span class="token punctuation">(</span>np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line">    ax<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line">    ax<span class="token punctuation">.</span>set_yticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">    ax<span class="token punctuation">.</span>set_xticklabels<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">    <span class="token keyword">return</span> ax
</span></code></pre></div><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">show_img<span class="token punctuation">(</span>img_scaled<span class="token punctuation">)</span>
</span></code></pre></div><p><code>&lt;AxesSubplot: &gt;</code></p><p><img alt="png" src="https://www.storminthecastle.com/img/01_classification_files/01_classification_25_1.png"/></p><p>We will want to be able to overlay the class labels on top of our image in the appropriate locations along with the bounding boxes for each object. Here are a couple of utility functions that allow us to draw text on top of an image and allow us to draw a rectangle on top of an image.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># draw an outline around the shape; used to add contrast to the text so we can read it easily</span>
</span><span class="code-line"><span class="token keyword">def</span> <span class="token function">draw_outline</span><span class="token punctuation">(</span>o<span class="token punctuation">,</span> lw<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    o<span class="token punctuation">.</span>set_path_effects<span class="token punctuation">(</span><span class="token punctuation">[</span>patheffects<span class="token punctuation">.</span>Stroke<span class="token punctuation">(</span>
</span><span class="code-line">        linewidth<span class="token operator">=</span>lw<span class="token punctuation">,</span> foreground<span class="token operator">=</span><span class="token string">&#x27;black&#x27;</span><span class="token punctuation">)</span><span class="token punctuation">,</span> patheffects<span class="token punctuation">.</span>Normal<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">
</span><span class="code-line"><span class="token comment"># draw text in the specified location along with an outline so that there is some contrast between the text and the image</span>
</span><span class="code-line"><span class="token keyword">def</span> <span class="token function">draw_text</span><span class="token punctuation">(</span>ax<span class="token punctuation">,</span> xy<span class="token punctuation">,</span> txt<span class="token punctuation">,</span> sz<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">&#x27;white&#x27;</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    text <span class="token operator">=</span> ax<span class="token punctuation">.</span>text<span class="token punctuation">(</span><span class="token operator">*</span>xy<span class="token punctuation">,</span> txt<span class="token punctuation">,</span>
</span><span class="code-line">        verticalalignment<span class="token operator">=</span><span class="token string">&#x27;top&#x27;</span><span class="token punctuation">,</span> color<span class="token operator">=</span>color<span class="token punctuation">,</span> fontsize<span class="token operator">=</span>sz<span class="token punctuation">,</span> weight<span class="token operator">=</span><span class="token string">&#x27;bold&#x27;</span><span class="token punctuation">)</span>
</span><span class="code-line">    draw_outline<span class="token punctuation">(</span>text<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
</span><span class="code-line">
</span><span class="code-line"><span class="token keyword">def</span> <span class="token function">draw_rect</span><span class="token punctuation">(</span>ax<span class="token punctuation">,</span> b<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">&#x27;white&#x27;</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    patch <span class="token operator">=</span> ax<span class="token punctuation">.</span>add_patch<span class="token punctuation">(</span>patches<span class="token punctuation">.</span>Rectangle<span class="token punctuation">(</span>b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">*</span>b<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> fill<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> edgecolor<span class="token operator">=</span>color<span class="token punctuation">,</span> lw<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</span><span class="code-line">    draw_outline<span class="token punctuation">(</span>patch<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
</span></code></pre></div><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token keyword">def</span> <span class="token function">get_cmap</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    color_norm  <span class="token operator">=</span> mcolors<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>vmin<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> vmax<span class="token operator">=</span>N<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
</span><span class="code-line">    <span class="token keyword">return</span> cmx<span class="token punctuation">.</span>ScalarMappable<span class="token punctuation">(</span>norm<span class="token operator">=</span>color_norm<span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">&#x27;Set3&#x27;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to_rgba
</span></code></pre></div><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># generate a list of different colors for rendering our bounding boxes</span>
</span><span class="code-line">num_colr <span class="token operator">=</span> <span class="token number">12</span>
</span><span class="code-line">cmap <span class="token operator">=</span> get_cmap<span class="token punctuation">(</span>num_colr<span class="token punctuation">)</span>
</span><span class="code-line">colr_list <span class="token operator">=</span> <span class="token punctuation">[</span>cmap<span class="token punctuation">(</span><span class="token builtin">float</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_colr<span class="token punctuation">)</span><span class="token punctuation">]</span>
</span></code></pre></div><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># draw an image along with it&#x27;s associated bounding boxes and class labels</span>
</span><span class="code-line"><span class="token keyword">def</span> <span class="token function">show_item</span><span class="token punctuation">(</span>im<span class="token punctuation">,</span> lbl_bbox<span class="token punctuation">,</span> figsize<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> ax<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    <span class="token keyword">if</span> <span class="token keyword">not</span> ax<span class="token punctuation">:</span> fig<span class="token punctuation">,</span>ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span>figsize<span class="token punctuation">)</span>
</span><span class="code-line">    ax <span class="token operator">=</span> show_img<span class="token punctuation">(</span>im<span class="token punctuation">,</span> ax<span class="token operator">=</span>ax<span class="token punctuation">)</span>
</span><span class="code-line">    <span class="token keyword">for</span> i<span class="token punctuation">,</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span>c<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>lbl_bbox<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lbl_bbox<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">        b <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token operator">*</span>b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>b<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">-</span>b<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span>b<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">-</span>b<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span>
</span><span class="code-line">        draw_rect<span class="token punctuation">(</span>ax<span class="token punctuation">,</span> b<span class="token punctuation">,</span> color<span class="token operator">=</span>colr_list<span class="token punctuation">[</span>i<span class="token operator">%</span>num_colr<span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">        draw_text<span class="token punctuation">(</span>ax<span class="token punctuation">,</span> b<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> c<span class="token punctuation">,</span> color<span class="token operator">=</span>colr_list<span class="token punctuation">[</span>i<span class="token operator">%</span>num_colr<span class="token punctuation">]</span><span class="token punctuation">)</span>
</span></code></pre></div><p>Let&#x27;s draw our scaled image along with it&#x27;s scaled bounding boxes and class labels.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">show_item<span class="token punctuation">(</span>img_scaled<span class="token punctuation">,</span>img_bbox_scaled<span class="token punctuation">)</span>
</span></code></pre></div><p><img alt="png" src="https://www.storminthecastle.com/img/01_classification_files/01_classification_32_0.png"/></p><p>By now you should have a pretty good understanding of what the dataset looks like. Our goal for this series of articles will be to use this dataset to develop and train an object detection model that can take an image as input and will output a list of class labels for each object within the image along with their respective bounding boxes. But we will do this incrementally in order to build up an intuition of how the system will work.</p><h2 id="decomposing-the-problem"><a href="#decomposing-the-problem" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Decomposing the problem</h2><p>Let&#x27;s start with a easier problem and work our way up from there. Let&#x27;s make a much simpler model, one that takes an image as input and predicts just a single object class for the image. let&#x27;s do this for the largest object present in each image. We can use our dataset and the bounding box information that we have to identify the largest object in each image and use that as a derived dataset to get us started.</p><p>Here&#x27;s a function that given a labeled bounding box sample will return the largest single bounding box along with it&#x27;s class label.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># utility function that takes a bounding box in the form of x1,y1,x2,y2 and returns it&#x27;s area (w*h)</span>
</span><span class="code-line"><span class="token keyword">def</span> <span class="token function">area</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">return</span> <span class="token punctuation">(</span>b<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token operator">-</span>b<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>b<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token operator">-</span>b<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</span><span class="code-line">
</span><span class="code-line"><span class="token comment"># zip the bounding boxes together with the object class;</span>
</span><span class="code-line"><span class="token comment"># sort it descending order by the size of the bounding;</span>
</span><span class="code-line"><span class="token comment"># return the first one (largest one)</span>
</span><span class="code-line"><span class="token keyword">def</span> <span class="token function">get_largest</span><span class="token punctuation">(</span>boxes<span class="token punctuation">)</span><span class="token punctuation">:</span>
</span><span class="code-line">    <span class="token keyword">return</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>L<span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span>boxes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>key<span class="token operator">=</span><span class="token keyword">lambda</span> b<span class="token punctuation">:</span> <span class="token operator">-</span>area<span class="token punctuation">(</span>b<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
</span></code></pre></div><p>Review our sample out of the dataset.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># our labled bounding box sample</span>
</span><span class="code-line">img_bbox_scaled
</span></code></pre></div><p><code>([[85.0, 37.0, 130.0, 122.0], [41.0, 47.0, 188.0, 206.0]], [&#x27;person&#x27;, &#x27;horse&#x27;])</code></p><p>Use the get_largest function to get the largest object for a given sample.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># get the largest object from our sample</span>
</span><span class="code-line">get_largest<span class="token punctuation">(</span>img_bbox_scaled<span class="token punctuation">)</span>
</span></code></pre></div><p><code>([41.0, 47.0, 188.0, 206.0], &#x27;horse&#x27;)</code></p><p>Now that we have a way to get the largest object for a sample we can just use a list comprehension to process all of the training metadata and produce a new training set that just contains the largest objects for each image.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">lrg_bbox <span class="token operator">=</span> <span class="token punctuation">[</span>get_largest<span class="token punctuation">(</span>boxes<span class="token punctuation">)</span> <span class="token keyword">for</span> boxes <span class="token keyword">in</span> lbl_bbox<span class="token punctuation">]</span>
</span></code></pre></div><p>Create a dictionary that we can use to look up we can look up our dependent variables (largest object and bounding box for each image) given the independent variable(img file name)</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">img2lrgbbox <span class="token operator">=</span> <span class="token builtin">dict</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>imgs<span class="token punctuation">,</span>lrg_bbox<span class="token punctuation">)</span><span class="token punctuation">)</span>
</span></code></pre></div><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># look at a sample from our dictionary</span>
</span><span class="code-line">k <span class="token operator">=</span> L<span class="token punctuation">(</span>img2lrgbbox<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">;</span> k<span class="token punctuation">,</span>img2lrgbbox<span class="token punctuation">[</span>k<span class="token punctuation">]</span>
</span></code></pre></div><p><code>(&#x27;000017.jpg&#x27;, ([89, 77, 403, 336], &#x27;horse&#x27;))</code></p><h2 id="training-a-classification-model"><a href="#training-a-classification-model" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Training a Classification Model</h2><p>Let&#x27;s setup and train a model that will classify the largest object within a given image. Fast AI makes this pretty quick and easy using the DataBlock API. For now we won&#x27;t use the bounding box information but will just use the class label from the dictionary that we created.</p><h3 id="setting-up-the-dataloaders"><a href="#setting-up-the-dataloaders" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Setting up the Dataloaders</h3><p>Define a getter for the DataBlock API. Given a training image file name will return the full path to the image file and the class label of the largest object in that image.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line"><span class="token comment"># returns [full path to image file, largest object class name]</span>
</span><span class="code-line">getters <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token keyword">lambda</span> o<span class="token punctuation">:</span> path<span class="token operator">/</span><span class="token string">&#x27;train&#x27;</span><span class="token operator">/</span>o<span class="token punctuation">,</span> <span class="token keyword">lambda</span> o<span class="token punctuation">:</span> img2lrgbbox<span class="token punctuation">[</span>o<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
</span></code></pre></div><p>Try out the Datablock getter providing an image file name from our dataset (k).</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">k<span class="token punctuation">,</span>getters<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">,</span>getters<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>k<span class="token punctuation">)</span>
</span></code></pre></div><p><code>(&#x27;000017.jpg&#x27;, Path(&#x27;/home/jr/.fastai/data/pascal_2007/train/000017.jpg&#x27;),&#x27;horse&#x27;)</code></p><p>A few image transforms. item_tfms will be used to make all of the dataset images the same size. batch_tfms will be used to procedurally create more training data to improve our model performance.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">item_tfms <span class="token operator">=</span> <span class="token punctuation">[</span>Resize<span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">,</span> method<span class="token operator">=</span><span class="token string">&#x27;squish&#x27;</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">]</span>
</span><span class="code-line">batch_tfms <span class="token operator">=</span> <span class="token punctuation">[</span>Rotate<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Flip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> Dihedral<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment"># Some basic image augmentions so that that our model get&#x27;s more input image diversity during training</span>
</span></code></pre></div><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">dblock <span class="token operator">=</span> DataBlock<span class="token punctuation">(</span>blocks<span class="token operator">=</span><span class="token punctuation">(</span>ImageBlock<span class="token punctuation">,</span> CategoryBlock<span class="token punctuation">)</span><span class="token punctuation">,</span>
</span><span class="code-line">                 getters<span class="token operator">=</span>getters<span class="token punctuation">,</span>
</span><span class="code-line">                 item_tfms<span class="token operator">=</span>item_tfms<span class="token punctuation">,</span>
</span><span class="code-line">                 batch_tfms<span class="token operator">=</span>batch_tfms<span class="token punctuation">)</span>
</span></code></pre></div><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">dls <span class="token operator">=</span> dblock<span class="token punctuation">.</span>dataloaders<span class="token punctuation">(</span>imgs<span class="token punctuation">,</span> bs <span class="token operator">=</span> <span class="token number">128</span><span class="token punctuation">)</span>
</span></code></pre></div><p>Fast AI will look at the dataset and collect all of the classes that our dataset contains which yields 20 classes as follows:</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">dls<span class="token punctuation">.</span>vocab<span class="token punctuation">,</span><span class="token builtin">len</span><span class="token punctuation">(</span>dls<span class="token punctuation">.</span>vocab<span class="token punctuation">)</span>
</span></code></pre></div><p><code>([&#x27;aeroplane&#x27;, &#x27;bicycle&#x27;, &#x27;bird&#x27;, &#x27;boat&#x27;, &#x27;bottle&#x27;, &#x27;bus&#x27;, &#x27;car&#x27;, &#x27;cat&#x27;, &#x27;chair&#x27;, &#x27;cow&#x27;, &#x27;diningtable&#x27;, &#x27;dog&#x27;, &#x27;horse&#x27;, &#x27;motorbike&#x27;, &#x27;person&#x27;, &#x27;pottedplant&#x27;, &#x27;sheep&#x27;, &#x27;sofa&#x27;, &#x27;train&#x27;, &#x27;tvmonitor&#x27;], 20)</code></p><p>Now that our data has been setup, Fast AI can show us a sample training batch that will be fed into the model for training. Showing the images and the ground truth labels for the largest object in the image.</p><p><em>Note: Image augmentations specified above will be randomly applied.</em></p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">dls<span class="token punctuation">.</span>show_batch<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span></code></pre></div><p><img alt="png" src="https://www.storminthecastle.com/img/01_classification_files/01_classification_57_0.png"/></p><p>We can now use the vision_learner API and specify our dataset, the model architecture that we&#x27;d like to use, and any training metrics that we&#x27;d like to see while training.</p><p>Here we&#x27;ll use resnet34 model which provides a pretty good mix of capacity and performance for our experiment. Here FAI uses transfer learning by default where the pretrained weights for the resnet34 model will be used.</p><p><em>Note: These pretrained weights were trained against the imagenet dataset and will enable our model to learn much faster.</em></p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">learn <span class="token operator">=</span> vision_learner<span class="token punctuation">(</span>dls<span class="token punctuation">,</span>resnet34<span class="token punctuation">,</span>metrics<span class="token operator">=</span>accuracy<span class="token punctuation">)</span>
</span></code></pre></div><p><code>/home/jr/anaconda3/envs/fastaip2_3/lib/python3.9/site-packages/torchvision/models/\_utils.py:208: UserWarning: The parameter &#x27;pretrained&#x27; is deprecated since 0.13 and may be removed in the future, please use &#x27;weights&#x27; instead. warnings.warn( /home/jr/anaconda3/envs/fastaip2_3/lib/python3.9/site-packages/torchvision/models/\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#x27;weights&#x27; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg)</code></p><p>Fast AI auto selects a suitable loss function for a classification model given that we specified the CategoryBlock via the DataBlock API. We can examine the selected loss function here.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">learn<span class="token punctuation">.</span>loss_func
</span></code></pre></div><p>FlattenedLoss of CrossEntropyLoss()</p><p>Let&#x27;s find a reasonble learning rate.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">lrs <span class="token operator">=</span> learn<span class="token punctuation">.</span>lr_find<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span><span class="code-line">lrs
</span></code></pre></div><p>SuggestedLRs(valley=0.0008317637839354575)</p><p><img alt="png" src="https://www.storminthecastle.com/img/01_classification_files/01_classification_63_3.png"/></p><h3 id="train-our-model"><a href="#train-our-model" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Train Our Model</h3><p>We&#x27;ll fit the model to our training data using the selected loss function. Here we&#x27;re using fine_tune since we are using the resnet34 pretrained weights.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">learn<span class="token punctuation">.</span>fine_tune<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span>base_lr<span class="token operator">=</span>lrs<span class="token punctuation">.</span>valley<span class="token punctuation">)</span>
</span></code></pre></div><table border="1" class="dataframe"><thead><tr><th>epoch</th> <th>train_loss</th> <th>valid_loss</th> <th>accuracy</th> <th>time</th></tr></thead><tbody><tr><td>0</td> <td>4.157558</td> <td>1.687271</td> <td>0.538000</td> <td>00:05</td></tr></tbody></table> <table border="1" class="dataframe"><thead><tr><th>epoch</th> <th>train_loss</th> <th>valid_loss</th> <th>accuracy</th> <th>time</th></tr></thead><tbody><tr><td>0</td> <td>2.541830</td> <td>1.374761</td> <td>0.622000</td> <td>00:06</td></tr><tr><td>1</td> <td>2.298766</td> <td>1.057605</td> <td>0.692000</td> <td>00:06</td></tr><tr><td>2</td> <td>1.984269</td> <td>0.888334</td> <td>0.746000</td> <td>00:06</td></tr><tr><td>3</td> <td>1.709419</td> <td>0.845897</td> <td>0.780000</td> <td>00:06</td></tr><tr><td>4</td> <td>1.484500</td> <td>0.803179</td> <td>0.774000</td> <td>00:06</td></tr><tr><td>5</td> <td>1.295198</td> <td>0.795235</td> <td>0.772000</td> <td>00:06</td></tr><tr><td>6</td> <td>1.144498</td> <td>0.789521</td> <td>0.786000</td> <td>00:06</td></tr><tr><td>7</td> <td>1.041366</td> <td>0.787113</td> <td>0.782000</td> <td>00:06</td></tr><tr><td>8</td> <td>0.958484</td> <td>0.785957</td> <td>0.780000</td> <td>00:06</td></tr><tr><td>9</td> <td>0.887360</td> <td>0.789864</td> <td>0.780000</td> <td>00:06</td></tr></tbody></table><p>We&#x27;re able to get to about 80% accuracy pretty quickly which isn&#x27;t too bad given that we&#x27;re trying to predict a single class label to represent the image. Most of the images have multiple objects and the largest objects aren&#x27;t centered which adds some amount of confusion.</p><p>Let&#x27;s look at a few predictions showing both the ground-truth label and the label predicted by our trained model.</p><div class="relative"><pre><code class="language-python code-highlight"><span class="code-line">learn<span class="token punctuation">.</span>show_results<span class="token punctuation">(</span><span class="token punctuation">)</span>
</span></code></pre></div><p><img alt="png" src="https://www.storminthecastle.com/img/01_classification_files/01_classification_67_2.png"/></p><h2 id="summary"><a href="#summary" aria-hidden="true" tabindex="-1"><span class="icon icon-link"></span></a>Summary</h2><p>In this article, we started explored the problem of object detection. We selected and explored a dataset that we can use for experimentation and we trained a model to classify the largest object within an image.</p><p>In my next article, I&#x27;ll start to show how we can climb one more rung up the ladder and walk you through how we can design a model to handle bounding boxes. Building a regression model that can predict the bounding box for the largest object in the image without the class label.</p><p>This article series leverages the current version of the <a target="_blank" rel="noopener noreferrer" href="https://github.com/fastai/fastai">Fast AI 2 library</a> a powerful machine learning framework built on top of PyTorch.</p><p><a target="_blank" rel="noopener noreferrer" href="https://www.storminthecastle.com/posts/02_bounding_box/">Read Part 2</a></p></div><div class="pt-6 pb-6 text-sm text-gray-700 dark:text-gray-300"><a target="_blank" rel="nofollow" href="https://mobile.twitter.com/search?q=https%3A%2F%2Ftailwind-nextjs-starter-blog.vercel.app%2Fblog%2F01_classification">Discuss on Twitter</a>  <a target="_blank" rel="noopener noreferrer" href="https://github.com/timlrx/tailwind-nextjs-starter-blog/blob/master/data/blog/01_classification.md">View on GitHub</a></div><div id="comment"></div></div><footer><div class="divide-gray-200 text-sm font-medium leading-5 dark:divide-gray-700 xl:col-start-1 xl:row-start-2 xl:divide-y"><div class="py-4 xl:py-8"><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400">Tags</h2><div class="flex flex-wrap"><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/object-detection">object-detection</a><a class="mr-3 text-sm font-medium uppercase text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/tags/code">code</a></div></div><div class="flex justify-between py-4 xl:block xl:space-y-8 xl:py-8"><div><h2 class="text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400">Next Article</h2><div class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400"><a href="/blog/posing-heads-with-stable-diffusion">Posing Heads with Stable Diffusion</a></div></div></div></div><div class="pt-4 xl:pt-8"><a class="text-primary-500 hover:text-primary-600 dark:hover:text-primary-400" href="/blog"> Back to the blog</a></div></footer></div></div></article></div></main><footer><div class="mt-16 flex flex-col items-center"><div class="mb-3 flex space-x-4"><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="mailto:johnrobinsn@gmail.com"><span class="sr-only">mail</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M2.003 5.884 10 9.882l7.997-3.998A2 2 0 0 0 16 4H4a2 2 0 0 0-1.997 1.884z"></path><path d="m18 8.118-8 4-8-4V14a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8.118z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://github.com/johnrobinsn"><span class="sr-only">github</span><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://www.linkedin.com/in/johnrobinsonsprofile/"><span class="sr-only">linkedin</span><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 0 1-2.063-2.065 2.064 2.064 0 1 1 2.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a><a class="text-sm text-gray-500 transition hover:text-gray-600" target="_blank" rel="noopener noreferrer" href="https://twitter.com/johnrobinsn"><span class="sr-only">twitter</span><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="fill-current text-gray-700 hover:text-blue-500 dark:text-gray-200 dark:hover:text-blue-400 h-6 w-6"><path d="M23.953 4.57a10 10 0 0 1-2.825.775 4.958 4.958 0 0 0 2.163-2.723c-.951.555-2.005.959-3.127 1.184a4.92 4.92 0 0 0-8.384 4.482C7.69 8.095 4.067 6.13 1.64 3.162a4.822 4.822 0 0 0-.666 2.475c0 1.71.87 3.213 2.188 4.096a4.904 4.904 0 0 1-2.228-.616v.06a4.923 4.923 0 0 0 3.946 4.827 4.996 4.996 0 0 1-2.212.085 4.936 4.936 0 0 0 4.604 3.417 9.867 9.867 0 0 1-6.102 2.105c-.39 0-.779-.023-1.17-.067a13.995 13.995 0 0 0 7.557 2.209c9.053 0 13.998-7.496 13.998-13.985 0-.21 0-.42-.015-.63A9.935 9.935 0 0 0 24 4.59z"></path></svg></a></div><div class="mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400"><div>John Robinson</div><div>  </div><div> 2023</div><div>  </div><a href="/">Stormin&#x27; the Castle Blog </a></div><div class="mb-8 text-sm text-gray-500 dark:text-gray-400"><a target="_blank" rel="noopener noreferrer" href="https://github.com/timlrx/tailwind-nextjs-starter-blog">Tailwind Nextjs Theme</a></div></div></footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"mdxSource":"var Component=(()=\u003e{var r=Object.create;var c=Object.defineProperty;var d=Object.getOwnPropertyDescriptor;var h=Object.getOwnPropertyNames;var p=Object.getPrototypeOf,m=Object.prototype.hasOwnProperty;var l=a=\u003ec(a,\"__esModule\",{value:!0});var u=(a,s)=\u003e()=\u003e(s||a((s={exports:{}}).exports,s),s.exports),N=(a,s)=\u003e{l(a);for(var t in s)c(a,t,{get:s[t],enumerable:!0})},k=(a,s,t)=\u003e{if(s\u0026\u0026typeof s==\"object\"||typeof s==\"function\")for(let n of h(s))!m.call(a,n)\u0026\u0026n!==\"default\"\u0026\u0026c(a,n,{get:()=\u003es[n],enumerable:!(t=d(s,n))||t.enumerable});return a},g=a=\u003ek(l(c(a!=null?r(p(a)):{},\"default\",a\u0026\u0026a.__esModule\u0026\u0026\"default\"in a?{get:()=\u003ea.default,enumerable:!0}:{value:a,enumerable:!0})),a);var o=u((x,i)=\u003e{i.exports=_jsx_runtime});var y={};N(y,{default:()=\u003ew,frontmatter:()=\u003eb});var e=g(o()),b={title:\"Object Detection from Scratch - Part 1\",date:new Date(1673537534e3),lastmod:\"2023-01-29\",tags:[\"object detection\",\"code\"],draft:!1};function f(a={}){let{wrapper:s}=a.components||{};return s?(0,e.jsx)(s,Object.assign({},a,{children:(0,e.jsx)(t,{})})):t();function t(){let n=Object.assign({p:\"p\",a:\"a\",pre:\"pre\",code:\"code\",span:\"span\",h2:\"h2\",img:\"img\",h3:\"h3\",em:\"em\"},a.components);return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)(\"img\",{src:\"https://www.storminthecastle.com/img/01_classification_files/computervision1_256.jpeg\"}),(0,e.jsx)(\"br\",{}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.a,{href:\"https://github.com/johnrobinsn/ObjectDetectionNotebooks/blob/main/nbs/01_classification.ipynb\",children:(0,e.jsx)(\"img\",{src:\"https://www.storminthecastle.com/img/github.svg\"})}),(0,e.jsx)(n.a,{href:\"https://colab.research.google.com/github/johnrobinsn/ObjectDetectionNotebooks/blob/main/nbs/01_classification.ipynb\",children:(0,e.jsx)(\"img\",{src:\"https://www.storminthecastle.com/img/colab.svg\"})})]}),(0,e.jsxs)(n.p,{children:[`This is the start of my new series, \"Object Detection from Scratch\", which is focused on building an intuition for how single-pass object detectors such as YOLO and SSD work. Object detection considers the problem of building a model that can take an image and detect multiple objects within that image; predicting not only the object classes but also the bounding boxes of those objects within the image. Single-pass detectors such as YOLO and SSD have solved this problem end-to-end performing the object detection task in a single forward inference pass of the model. If what I'm describing is not clear, here is a fun video of `,(0,e.jsx)(n.a,{href:\"https://www.youtube.com/watch?v=MPU2HistivI\",children:\"YOLO in action\"}),\".\"]}),(0,e.jsx)(n.p,{children:\"In this series, I will incrementally build up a YOLO/SSD (Single Shot Detector) object model with just PyTorch and the current version of the FastAI 2 library. Both SSD and YOLO allow for single pass inference and can run efficiently on fairly low-end hardware allowing for realtime object detection for video content etc.\"}),(0,e.jsxs)(n.p,{children:[\"In order to learn about object detection we'll need a dataset. We'll be focused on using the \",(0,e.jsx)(n.a,{href:\"http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html\",children:\"Pascal VOC dataset (2007)\"}),\". In total this dataset includes almost 10k images, containing about 24k annotated objects covering 20 different object classes. Each image is annotated with some number of object class labels and their cooresponding bounding boxes.\"]}),(0,e.jsx)(n.p,{children:\"The notebooks in this series are designed to be easily used within Google Colab (free) or if you have your own GPU I'd recommend using conda to setup an environment with Python \u003e3.9 and pip install fastai.\"}),(0,e.jsx)(n.p,{children:'To easily open this notebook just click on the \"Open in Colab\" button above.'}),(0,e.jsx)(n.p,{children:\"The FastAI (FAI) library makes it easy to download the Pascal VOC dataset and access the object annotations.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" fastai\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"vision\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"all\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),\" \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"*\"}),`\n`]})})}),(0,e.jsx)(n.pre,{children:(0,e.jsxs)(n.code,{className:\"code-highlight\",children:[(0,e.jsx)(n.span,{className:\"code-line\",children:`/home/jr/anaconda3/envs/fastaip2_3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n`}),(0,e.jsx)(n.span,{className:\"code-line\",children:`warn(f\"Failed to load image Python extension: {e}\")\n`})]})}),(0,e.jsx)(n.p,{children:\"Download the dataset using FAI's untar_data function and take a quick look at the files that have been downloaded.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"path \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" untar_data\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"URLs\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"PASCAL_2007\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"path\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"ls\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"(#8) [Path('/home/jr/.fastai/data/pascal_2007/train.json'),Path('/home/jr/.fastai/data/pascal_2007/test.csv'),Path('/home/jr/.fastai/data/pascal_2007/train.csv'),Path('/home/jr/.fastai/data/pascal_2007/train'),Path('/home/jr/.fastai/data/pascal_2007/test.json'),Path('/home/jr/.fastai/data/pascal_2007/valid.json'),Path('/home/jr/.fastai/data/pascal_2007/segmentation'),Path('/home/jr/.fastai/data/pascal_2007/test')]\"})}),(0,e.jsxs)(n.h2,{id:\"exploring-the-dataset\",children:[(0,e.jsx)(n.a,{href:\"#exploring-the-dataset\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Exploring the Dataset\"]}),(0,e.jsx)(n.p,{children:\"FAI includes a get_annotations function that can parse COCO style bounding box annotations which the VOC dataset uses. Here we'll just load the data associated with the train set (train.json). The function will return a tuple of lists. The first list will contain the image file names in the training set. The second list will contain cooresponding bounding boxes and object class labels for those images.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"imgs\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"lbl_bbox \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" get_annotations\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"path\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'train.json'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"imgs\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"lbl_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"(2501, 2501)\"})}),(0,e.jsx)(n.p,{children:\"The dataset provides over 2500 training labeled (classes and bounding boxes). Here is a single example that demonstrates an image that has more than one object annotated. Showing two bounding boxes in image-space coordinates along with their respective object class labels.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"imgs\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"lbl_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\"  \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# show a single example\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"('000017.jpg', ([[184, 61, 279, 199], [89, 77, 403, 336]], ['person', 'horse']))\"})}),(0,e.jsx)(n.p,{children:\"The bounding boxes consist of four numbers, the first two numbers represent the xy coordinates of the upper left corner of the bounding box and the second two numbers represent the xy coordinates of the lower right corner of the bounding box.\"}),(0,e.jsxs)(n.h2,{id:\"visualize-the-dataset\",children:[(0,e.jsx)(n.a,{href:\"#visualize-the-dataset\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Visualize the Dataset\"]}),(0,e.jsx)(n.p,{children:\"Using the matplotlib library let's visualize an example from the training set.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),\" matplotlib\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"colors \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"as\"}),` mcolors\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),\" matplotlib\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"cm \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"as\"}),` cmx\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"from\"}),\" matplotlib \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"import\"}),\" patches\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),` patheffects\n`]})]})}),(0,e.jsx)(n.p,{children:\"Let's grab a sample out of the training set to explore.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# grab our example\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"img_file\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"img_bbox \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" imgs\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"lbl_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"img_file\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`img_bbox\n`]})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"('000017.jpg', ([[184, 61, 279, 199], [89, 77, 403, 336]], ['person', 'horse']))\"})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# load the image using PIL\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"img \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" Image\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"open\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"path\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),(0,e.jsxs)(n.span,{className:\"token string-interpolation\",children:[(0,e.jsx)(n.span,{className:\"token string\",children:\"f'train/\"}),(0,e.jsxs)(n.span,{className:\"token interpolation\",children:[(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"{\"}),\"img_file\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"}\"})]}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'\"})]}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`img\n`})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{alt:\"png\",src:\"https://www.storminthecastle.com/img/01_classification_files/01_classification_14_0.png\"})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"h\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"w \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" img\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),`shape\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"h\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"w \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# the height and width of the image; Note that height is typically specified first for numpy and PIL\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"(364, 480)\"})}),(0,e.jsx)(n.p,{children:\"The images in the dataset come in different sizes and shapes (but all 3 channel RGB images). In order to be able to use a GPU to accelerate training, we will want to work with a batch of images all in one go. This means that all of our images will need to be of the same size so that we can stack a batch into a single tensor. A fairly typical size to work with is 224x224 pixels. One important consideration is that since our bounding boxes are defined in image space coordinates, we really don't want to crop the images since we might cut off our bounding boxes, therefore we opt to squish the images instead.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"SIZE\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"224\"}),`\n`]})})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# resample/rescale the example image by squishing\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"img_scaled \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" img\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"resize\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"SIZE\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"SIZE\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`img_scaled\n`})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{alt:\"png\",src:\"https://www.storminthecastle.com/img/01_classification_files/01_classification_18_0.png\"})}),(0,e.jsx)(n.p,{children:\"Given the task of object detection our independent variable are the images and the dependent variables are the object classes and the object bounding boxes. Given an image, we will want our model to predict a class label for each object in the image along with a bounding box for each object. One wrinkle however is that our bounding box (dependent variable) is defined in the coordinate space of our image (independent variable). So if we scale the input image we must also be careful to scale the bounding boxes along with it. Since the image has been squished to be of size 224x224 pixels we need to squish (scale) the dimensions our bounding boxes for that sample by the same amount. Here I show the steps required to squish the dimensions of the bounding boxes for our sample.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# calculate how how much we've scaled each dimension of the image\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"yscale\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"xscale \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" h\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),\"SIZE\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"w\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),`SIZE\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"yscale\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`xscale\n`]})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"(1.625, 2.142857142857143)\"})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# apply the same scale factor to each coordinate of the bounding box\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"img_bbox_scaled \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"x1\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"//\"}),\"xscale\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"y1\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"//\"}),\"yscale\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"x2\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"//\"}),\"xscale\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"y2\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"//\"}),\"yscale\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" x1\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"y1\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"x2\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"y2 \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" img_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`img_bbox_scaled\n`})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"[[85.0, 37.0, 130.0, 122.0], [41.0, 47.0, 188.0, 206.0]]\"})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# reconstruct the labled set of bounding boxes with the scaled bounding boxes for our example\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"img_bbox_scaled \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"img_bbox_scaled\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"img_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`img_bbox_scaled\n`})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"([[85.0, 37.0, 130.0, 122.0], [41.0, 47.0, 188.0, 206.0]], ['person', 'horse'])\"})}),(0,e.jsx)(n.p,{children:\"Here is a small utility function for displaying an image in such a way that we can layer on some additional annotations.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"show_img\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"im\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" figsize\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token boolean\",children:\"None\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" ax\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token boolean\",children:\"None\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"if\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"not\"}),\" ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" fig\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"ax \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" plt\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"subplots\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"figsize\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"figsize\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"imshow\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"im\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"set_xticks\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"np\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"linspace\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"224\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"8\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"set_yticks\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"np\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"linspace\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"224\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"8\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"grid\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"set_yticklabels\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"set_xticklabels\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"return\"}),` ax\n`]})]})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"show_img\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"img_scaled\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"\u003cAxesSubplot: \u003e\"})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{alt:\"png\",src:\"https://www.storminthecastle.com/img/01_classification_files/01_classification_25_1.png\"})}),(0,e.jsx)(n.p,{children:\"We will want to be able to overlay the class labels on top of our image in the appropriate locations along with the bounding boxes for each object. Here are a couple of utility functions that allow us to draw text on top of an image and allow us to draw a rectangle on top of an image.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# draw an outline around the shape; used to add contrast to the text so we can read it easily\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"draw_outline\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"o\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" lw\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    o\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"set_path_effects\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"patheffects\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"Stroke\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"        linewidth\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"lw\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" foreground\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'black'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" patheffects\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"Normal\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# draw text in the specified location along with an outline so that there is some contrast between the text and the image\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"draw_text\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" xy\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" txt\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" sz\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"14\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" color\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'white'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    text \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"text\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"*\"}),\"xy\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" txt\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"        verticalalignment\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'top'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" color\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"color\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" fontsize\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"sz\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" weight\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'bold'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    draw_outline\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"text\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"draw_rect\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" color\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'white'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    patch \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"add_patch\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"patches\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"Rectangle\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"*\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" fill\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token boolean\",children:\"False\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" edgecolor\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"color\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" lw\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    draw_outline\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"patch\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"4\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"get_cmap\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"N\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    color_norm  \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" mcolors\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"Normalize\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"vmin\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" vmax\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"N\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"return\"}),\" cmx\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"ScalarMappable\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"norm\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"color_norm\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" cmap\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'Set3'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),`to_rgba\n`]})]})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# generate a list of different colors for rendering our bounding boxes\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"num_colr \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"12\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"cmap \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" get_cmap\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"num_colr\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"colr_list \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"cmap\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"float\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"x\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" x \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"range\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"num_colr\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]})]})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# draw an image along with it's associated bounding boxes and class labels\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"show_item\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"im\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" lbl_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" figsize\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token boolean\",children:\"None\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" ax\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token boolean\",children:\"None\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"if\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"not\"}),\" ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" fig\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"ax \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" plt\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"subplots\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"figsize\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"figsize\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    ax \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" show_img\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"im\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" ax\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" i\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"c\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"enumerate\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"zip\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"lbl_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" lbl_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"        b \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"*\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"+\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"3\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"+\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"        draw_rect\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" color\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"colr_list\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"i\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"%\"}),\"num_colr\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"        draw_text\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"ax\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" c\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" color\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"colr_list\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"i\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"%\"}),\"num_colr\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:\"Let's draw our scaled image along with it's scaled bounding boxes and class labels.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"show_item\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"img_scaled\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"img_bbox_scaled\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{alt:\"png\",src:\"https://www.storminthecastle.com/img/01_classification_files/01_classification_32_0.png\"})}),(0,e.jsx)(n.p,{children:\"By now you should have a pretty good understanding of what the dataset looks like. Our goal for this series of articles will be to use this dataset to develop and train an object detection model that can take an image as input and will output a list of class labels for each object within the image along with their respective bounding boxes. But we will do this incrementally in order to build up an intuition of how the system will work.\"}),(0,e.jsxs)(n.h2,{id:\"decomposing-the-problem\",children:[(0,e.jsx)(n.a,{href:\"#decomposing-the-problem\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Decomposing the problem\"]}),(0,e.jsx)(n.p,{children:\"Let's start with a easier problem and work our way up from there. Let's make a much simpler model, one that takes an image as input and predicts just a single object class for the image. let's do this for the largest object present in each image. We can use our dataset and the bounding box information that we have to identify the largest object in each image and use that as a derived dataset to get us started.\"}),(0,e.jsx)(n.p,{children:\"Here's a function that given a labeled bounding box sample will return the largest single bounding box along with it's class label.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# utility function that takes a bounding box in the form of x1,y1,x2,y2 and returns it's area (w*h)\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"area\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"return\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"2\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"*\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"3\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`\n`}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# zip the bounding boxes together with the object class;\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# sort it descending order by the size of the bounding;\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# return the first one (largest one)\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token keyword\",children:\"def\"}),\" \",(0,e.jsx)(n.span,{className:\"token function\",children:\"get_largest\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"boxes\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"    \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"return\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"sorted\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"L\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"zip\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"*\"}),\"boxes\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"key\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token keyword\",children:\"lambda\"}),\" b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"-\"}),\"area\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"b\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:\"Review our sample out of the dataset.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# our labled bounding box sample\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`img_bbox_scaled\n`})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"([[85.0, 37.0, 130.0, 122.0], [41.0, 47.0, 188.0, 206.0]], ['person', 'horse'])\"})}),(0,e.jsx)(n.p,{children:\"Use the get_largest function to get the largest object for a given sample.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# get the largest object from our sample\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"get_largest\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"img_bbox_scaled\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"([41.0, 47.0, 188.0, 206.0], 'horse')\"})}),(0,e.jsx)(n.p,{children:\"Now that we have a way to get the largest object for a sample we can just use a list comprehension to process all of the training metadata and produce a new training set that just contains the largest objects for each image.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"lrg_bbox \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"get_largest\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"boxes\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"for\"}),\" boxes \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"in\"}),\" lbl_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:\"Create a dictionary that we can use to look up we can look up our dependent variables (largest object and bounding box for each image) given the independent variable(img file name)\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"img2lrgbbox \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token builtin\",children:\"dict\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"zip\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"imgs\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"lrg_bbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# look at a sample from our dictionary\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"k \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" L\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"img2lrgbbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\";\"}),\" k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"img2lrgbbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"('000017.jpg', ([89, 77, 403, 336], 'horse'))\"})}),(0,e.jsxs)(n.h2,{id:\"training-a-classification-model\",children:[(0,e.jsx)(n.a,{href:\"#training-a-classification-model\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Training a Classification Model\"]}),(0,e.jsx)(n.p,{children:\"Let's setup and train a model that will classify the largest object within a given image. Fast AI makes this pretty quick and easy using the DataBlock API. For now we won't use the bounding box information but will just use the class label from the dictionary that we created.\"}),(0,e.jsxs)(n.h3,{id:\"setting-up-the-dataloaders\",children:[(0,e.jsx)(n.a,{href:\"#setting-up-the-dataloaders\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Setting up the Dataloaders\"]}),(0,e.jsx)(n.p,{children:\"Define a getter for the DataBlock API. Given a training image file name will return the full path to the image file and the class label of the largest object in that image.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[(0,e.jsx)(n.span,{className:\"token comment\",children:\"# returns [full path to image file, largest object class name]\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"getters \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token keyword\",children:\"lambda\"}),\" o\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" path\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'train'\"}),(0,e.jsx)(n.span,{className:\"token operator\",children:\"/\"}),\"o\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" \",(0,e.jsx)(n.span,{className:\"token keyword\",children:\"lambda\"}),\" o\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\":\"}),\" img2lrgbbox\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"o\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]})]})}),(0,e.jsx)(n.p,{children:\"Try out the Datablock getter providing an image file name from our dataset (k).\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"getters\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"0\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"getters\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"1\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"k\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"('000017.jpg', Path('/home/jr/.fastai/data/pascal_2007/train/000017.jpg'),'horse')\"})}),(0,e.jsx)(n.p,{children:\"A few image transforms. item_tfms will be used to make all of the dataset images the same size. batch_tfms will be used to procedurally create more training data to improve our model performance.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"item_tfms \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"Resize\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"224\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" method\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token string\",children:\"'squish'\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"batch_tfms \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"[\"}),\"Rotate\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"10\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" Flip\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" Dihedral\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"]\"}),\"  \",(0,e.jsx)(n.span,{className:\"token comment\",children:\"# Some basic image augmentions so that that our model get's more input image diversity during training\"}),`\n`]})]})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"dblock \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" DataBlock\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"blocks\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"ImageBlock\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" CategoryBlock\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"                 getters\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"getters\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"                 item_tfms\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"item_tfms\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),`\n`]}),(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"                 batch_tfms\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"batch_tfms\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})]})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"dls \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" dblock\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"dataloaders\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"imgs\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\" bs \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" \",(0,e.jsx)(n.span,{className:\"token number\",children:\"128\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:\"Fast AI will look at the dataset and collect all of the classes that our dataset contains which yields 20 classes as follows:\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"dls\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"vocab\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),(0,e.jsx)(n.span,{className:\"token builtin\",children:\"len\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dls\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"vocab\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"(['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'], 20)\"})}),(0,e.jsx)(n.p,{children:\"Now that our data has been setup, Fast AI can show us a sample training batch that will be fed into the model for training. Showing the images and the ground truth labels for the largest object in the image.\"}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.em,{children:\"Note: Image augmentations specified above will be randomly applied.\"})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"dls\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"show_batch\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{alt:\"png\",src:\"https://www.storminthecastle.com/img/01_classification_files/01_classification_57_0.png\"})}),(0,e.jsx)(n.p,{children:\"We can now use the vision_learner API and specify our dataset, the model architecture that we'd like to use, and any training metrics that we'd like to see while training.\"}),(0,e.jsx)(n.p,{children:\"Here we'll use resnet34 model which provides a pretty good mix of capacity and performance for our experiment. Here FAI uses transfer learning by default where the pretrained weights for the resnet34 model will be used.\"}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.em,{children:\"Note: These pretrained weights were trained against the imagenet dataset and will enable our model to learn much faster.\"})}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"learn \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" vision_learner\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),\"dls\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"resnet34\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"metrics\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"accuracy\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.code,{children:\"/home/jr/anaconda3/envs/fastaip2_3/lib/python3.9/site-packages/torchvision/models/\\\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead. warnings.warn( /home/jr/anaconda3/envs/fastaip2_3/lib/python3.9/site-packages/torchvision/models/\\\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg)\"})}),(0,e.jsx)(n.p,{children:\"Fast AI auto selects a suitable loss function for a classification model given that we specified the CategoryBlock via the DataBlock API. We can examine the selected loss function here.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"learn\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),`loss_func\n`]})})}),(0,e.jsx)(n.p,{children:\"FlattenedLoss of CrossEntropyLoss()\"}),(0,e.jsx)(n.p,{children:\"Let's find a reasonble learning rate.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsxs)(n.code,{className:\"language-python code-highlight\",children:[(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"lrs \",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\" learn\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"lr_find\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]}),(0,e.jsx)(n.span,{className:\"code-line\",children:`lrs\n`})]})}),(0,e.jsx)(n.p,{children:\"SuggestedLRs(valley=0.0008317637839354575)\"}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{alt:\"png\",src:\"https://www.storminthecastle.com/img/01_classification_files/01_classification_63_3.png\"})}),(0,e.jsxs)(n.h3,{id:\"train-our-model\",children:[(0,e.jsx)(n.a,{href:\"#train-our-model\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Train Our Model\"]}),(0,e.jsx)(n.p,{children:\"We'll fit the model to our training data using the selected loss function. Here we're using fine_tune since we are using the resnet34 pretrained weights.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"learn\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"fine_tune\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token number\",children:\"10\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\",\"}),\"base_lr\",(0,e.jsx)(n.span,{className:\"token operator\",children:\"=\"}),\"lrs\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"valley\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsxs)(\"table\",{border:\"1\",className:\"dataframe\",children:[(0,e.jsx)(\"thead\",{children:(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"th\",{children:\"epoch\"}),\" \",(0,e.jsx)(\"th\",{children:\"train_loss\"}),\" \",(0,e.jsx)(\"th\",{children:\"valid_loss\"}),\" \",(0,e.jsx)(\"th\",{children:\"accuracy\"}),\" \",(0,e.jsx)(\"th\",{children:\"time\"})]})}),(0,e.jsx)(\"tbody\",{children:(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"0\"}),\" \",(0,e.jsx)(\"td\",{children:\"4.157558\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.687271\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.538000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:05\"})]})})]}),\" \",(0,e.jsxs)(\"table\",{border:\"1\",className:\"dataframe\",children:[(0,e.jsx)(\"thead\",{children:(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"th\",{children:\"epoch\"}),\" \",(0,e.jsx)(\"th\",{children:\"train_loss\"}),\" \",(0,e.jsx)(\"th\",{children:\"valid_loss\"}),\" \",(0,e.jsx)(\"th\",{children:\"accuracy\"}),\" \",(0,e.jsx)(\"th\",{children:\"time\"})]})}),(0,e.jsxs)(\"tbody\",{children:[(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"0\"}),\" \",(0,e.jsx)(\"td\",{children:\"2.541830\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.374761\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.622000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"1\"}),\" \",(0,e.jsx)(\"td\",{children:\"2.298766\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.057605\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.692000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"2\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.984269\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.888334\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.746000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"3\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.709419\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.845897\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.780000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"4\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.484500\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.803179\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.774000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"5\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.295198\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.795235\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.772000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"6\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.144498\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.789521\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.786000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"7\"}),\" \",(0,e.jsx)(\"td\",{children:\"1.041366\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.787113\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.782000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"8\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.958484\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.785957\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.780000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]}),(0,e.jsxs)(\"tr\",{children:[(0,e.jsx)(\"td\",{children:\"9\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.887360\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.789864\"}),\" \",(0,e.jsx)(\"td\",{children:\"0.780000\"}),\" \",(0,e.jsx)(\"td\",{children:\"00:06\"})]})]})]}),(0,e.jsx)(n.p,{children:\"We're able to get to about 80% accuracy pretty quickly which isn't too bad given that we're trying to predict a single class label to represent the image. Most of the images have multiple objects and the largest objects aren't centered which adds some amount of confusion.\"}),(0,e.jsx)(n.p,{children:\"Let's look at a few predictions showing both the ground-truth label and the label predicted by our trained model.\"}),(0,e.jsx)(n.pre,{className:\"language-python\",children:(0,e.jsx)(n.code,{className:\"language-python code-highlight\",children:(0,e.jsxs)(n.span,{className:\"code-line\",children:[\"learn\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\".\"}),\"show_results\",(0,e.jsx)(n.span,{className:\"token punctuation\",children:\"(\"}),(0,e.jsx)(n.span,{className:\"token punctuation\",children:\")\"}),`\n`]})})}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{alt:\"png\",src:\"https://www.storminthecastle.com/img/01_classification_files/01_classification_67_2.png\"})}),(0,e.jsxs)(n.h2,{id:\"summary\",children:[(0,e.jsx)(n.a,{href:\"#summary\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(n.span,{className:\"icon icon-link\"})}),\"Summary\"]}),(0,e.jsx)(n.p,{children:\"In this article, we started explored the problem of object detection. We selected and explored a dataset that we can use for experimentation and we trained a model to classify the largest object within an image.\"}),(0,e.jsx)(n.p,{children:\"In my next article, I'll start to show how we can climb one more rung up the ladder and walk you through how we can design a model to handle bounding boxes. Building a regression model that can predict the bounding box for the largest object in the image without the class label.\"}),(0,e.jsxs)(n.p,{children:[\"This article series leverages the current version of the \",(0,e.jsx)(n.a,{href:\"https://github.com/fastai/fastai\",children:\"Fast AI 2 library\"}),\" a powerful machine learning framework built on top of PyTorch.\"]}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.a,{href:\"https://www.storminthecastle.com/posts/02_bounding_box/\",children:\"Read Part 2\"})})]})}}var w=f;return y;})();\n;return Component;","toc":[{"value":"Exploring the Dataset","url":"#exploring-the-dataset","depth":2},{"value":"Visualize the Dataset","url":"#visualize-the-dataset","depth":2},{"value":"Decomposing the problem","url":"#decomposing-the-problem","depth":2},{"value":"Training a Classification Model","url":"#training-a-classification-model","depth":2},{"value":"Setting up the Dataloaders","url":"#setting-up-the-dataloaders","depth":3},{"value":"Train Our Model","url":"#train-our-model","depth":3},{"value":"Summary","url":"#summary","depth":2}],"frontMatter":{"readingTime":{"text":"16 min read","minutes":15.175,"time":910500,"words":3035},"slug":"01_classification","fileName":"01_classification.md","title":"Object Detection from Scratch - Part 1","date":"2023-01-12T15:32:14.000Z","lastmod":"2023-01-29","tags":["object detection","code"],"draft":false}},"authorDetails":[{"readingTime":{"text":"1 min read","minutes":0.495,"time":29700,"words":99},"slug":["default"],"fileName":"default.md","name":"John Robinson","avatar":"/static/images/avatar.jpeg","occupation":"Software Engineering \u0026 Architecture Executive","company":"Consultant for LiquidThought LLC","email":"johnrobinsn@gmail.com","github":"https://github.com/johnrobinsn","twitter":"https://twitter.com/johnrobinsn","linkedin":"https://www.linkedin.com/in/johnrobinsonsprofile/","date":null}],"prev":null,"next":{"title":"Posing Heads with Stable Diffusion","date":"2023-01-29T15:32:14.000Z","lastmod":"2023-01-29","tags":["Diffusion","MID-U","miniai"],"draft":false,"canonicalUrl":"https://tailwind-nextjs-starter-blog.vercel.app/blog/new-features-in-v1/","slug":"posing-heads-with-stable-diffusion"}},"__N_SSG":true},"page":"/blog/[...slug]","query":{"slug":["01_classification"]},"buildId":"mTxI35AS32IW1tSwy78Sx","assetPrefix":"https://pinkdragon1000.github.io/StorminTheCastle","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>